{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catchup with Past Workshops\n",
    "\n",
    "This workshop will have less content than usual so that you have a chance to catch up with previous workshops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification for Named Entity Recognition\n",
    "\n",
    "The lectures of week 6 described several possible features to train a classifier for the task of named entity recognition. Implement feature extractors that focus on some of these features and train a Naive Bayes classifier. Use the training and test files from the CONLL 2002 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['esp.testa', 'esp.testb', 'esp.train', 'ned.testa', 'ned.testb', 'ned.train']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2002.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = conll2002.iob_sents('esp.train')\n",
    "test = conll2002.iob_sents('esp.testa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Distribution of IOB tags\n",
    "\n",
    "Calculate the distribution of IOB tags both in the train and the test set and confirm that the distributions are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Feature extractor\n",
    "\n",
    "Implement an NLTK feature extractor that extracts the following features of a word:\n",
    "\n",
    "* The Part of speech (this is the second element of the input data).\n",
    "* True if the first letter is a capital letter, False otherwise.\n",
    "* True if the word contains a digit, False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Classification\n",
    "\n",
    "Train a NLTK Naive Bayes classifier with the above features and test the accuracy on the test set. Compare the results with the system provided in the NER notebook of the lectures of week 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercises\n",
    "\n",
    "Optionally, attempt the following exercises:\n",
    "\n",
    "1. Implement additional features based, for example, on the ones listed on the lectures.\n",
    "2. Use the sklearn multinomial Naive Bayes and compare results.\n",
    "3. Try with other classifiers, such as KNN, NLTK's [MaxEnt classifier](http://www.nltk.org/_modules/nltk/classify/maxent.html) and sklearn's [logistic regression classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). These classifiers are more powerful than Naive Bayes and scale well to large feature sets. Read, for example, the [NLTK book section](http://www.nltk.org/book/ch06.html#maximum-entropy-classifiers) and [sklearn's documentation on logistic regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) for further information on how these work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
