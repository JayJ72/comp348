{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration\n",
    "\n",
    "The following demonstration will use the training set of the OHSUMED corpus. This training set was used in the Filtering Track of the 9th edition of the Text REtrieval Conference (TREC-9). We will use it for the information retrieval exercises of this workshop. Download and unzip [ohsumed.zip] into the same folder as this notebook. \n",
    "\n",
    "To help you read the data, we are providing the file ohsumed.py (in the zip file above) that has a simple API to the data. When you import it at the Python prompt, it will provide the following variables:\n",
    "\n",
    "\n",
    "1. `index`: a dictionary with document IDs as keys, and document text as values.\n",
    "2. `questions`: a dictionary with query IDs as keys, and query text as values.\n",
    "3. `answers`: a dictionary with query IDs as keys, and a set with the IDs of known relevant documents for evaluation purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading OHSUMED data\n"
     ]
    }
   ],
   "source": [
    "import ohsumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54710"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index\n",
    "\n",
    "We are going to build an inverted index of the non-stop words with frequency higher than 5.\n",
    "\n",
    "The following code reads the files and creates a counter of all words in the corpus. We will use NLTK's word tokeniser (read the beginning of [chapter 3 of NLTK's book](http://www.nltk.org/book/ch03.html#processing-raw-text)) to convert each document into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, collections\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "wordcounter = collections.Counter([w.lower() for k in ohsumed.index\n",
    "                                             for w in nltk.word_tokenize(ohsumed.index[k])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the inverted index of all non-stop words with frequency higher than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inverted = dict()\n",
    "for d in ohsumed.index:\n",
    "    for w in nltk.word_tokenize(ohsumed.index[d]):\n",
    "        w = w.lower()\n",
    "        if w in stop or wordcounter[w] <= 5:\n",
    "            continue\n",
    "        if w in inverted:\n",
    "            inverted[w].add(d)\n",
    "        else:\n",
    "            inverted[w] = set([d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stroke-prone',\n",
       " 'uncomfortable',\n",
       " 'll',\n",
       " 'microseconds',\n",
       " 'fg',\n",
       " 'neomucosal',\n",
       " 'b-dna',\n",
       " 't3-t6-',\n",
       " 'breastfeeding',\n",
       " 'drug-resistant']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inverted.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87124462',\n",
       " '87179236',\n",
       " '87193011',\n",
       " '87249065',\n",
       " '87249082',\n",
       " '87249085',\n",
       " '87249096',\n",
       " '87264613',\n",
       " '87306829'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted['stroke-prone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code saves the inverted index into a pickle file. This way we do not need to recompute the inverted index again. Read [Python's documentation on pickle files](https://docs.python.org/3/library/pickle.html) for more detail. Note that the file we created is opened for writing in binary mode, following the advice of a [stackoverflow post about saving pickle files](http://stackoverflow.com/questions/13906623/using-pickle-dump-typeerror-must-be-str-not-bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inverted.pickle','wb') as f:\n",
    "    pickle.dump(inverted,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads the pickled file and returns the list of documents that maches this Boolean query:\n",
    "1. (menopausal OR pregnant) AND woman AND NOT healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('inverted.pickle','rb') as f:\n",
    "    inverted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87060673',\n",
       " '87066899',\n",
       " '87097274',\n",
       " '87097518',\n",
       " '87099263',\n",
       " '87114245',\n",
       " '87117852',\n",
       " '87128881',\n",
       " '87134330',\n",
       " '87138205',\n",
       " '87153548',\n",
       " '87153568',\n",
       " '87169457',\n",
       " '87185313',\n",
       " '87226668',\n",
       " '87231479',\n",
       " '87235637',\n",
       " '87251241',\n",
       " '87252385',\n",
       " '87261426',\n",
       " '87281235',\n",
       " '87290433',\n",
       " '87296136',\n",
       " '87316210',\n",
       " '87316220',\n",
       " '87316328',\n",
       " '87324028',\n",
       " '87325497'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inverted['menopausal'] | inverted['pregnant']) & inverted['woman'] - inverted['healthy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code builds an inverted index using the stems, not the whole words. Before stemming, all stop words are removed. After stemming, all words with frequency higher than 5 are ignored. For stemming the words, we will use the [Porter stemmer provided by NLTK](http://nltk.org/book/ch03#stemmers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "stemcounter = collections.Counter([porter.stem(w.lower()) for k in ohsumed.index\n",
    "                                   for w in nltk.word_tokenize(ohsumed.index[k])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stem_inverted = dict()\n",
    "for d in ohsumed.index.keys():\n",
    "    for w in nltk.word_tokenize(ohsumed.index[d]):\n",
    "        w = w.lower()\n",
    "        stem = porter.stem(w)\n",
    "        if w in stop or stemcounter[stem] <= 5:\n",
    "            continue\n",
    "        if stem in stem_inverted:\n",
    "            stem_inverted[stem].add(d)\n",
    "        else:\n",
    "            stem_inverted[stem] = set([d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30128"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23684"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stem_inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same Boolean query on this new inverted index. Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87054746',\n",
       " '87060673',\n",
       " '87066899',\n",
       " '87097274',\n",
       " '87097518',\n",
       " '87099263',\n",
       " '87114238',\n",
       " '87114241',\n",
       " '87114243',\n",
       " '87114245',\n",
       " '87114246',\n",
       " '87117852',\n",
       " '87128881',\n",
       " '87134330',\n",
       " '87138205',\n",
       " '87153548',\n",
       " '87153568',\n",
       " '87169457',\n",
       " '87185313',\n",
       " '87189233',\n",
       " '87210461',\n",
       " '87226668',\n",
       " '87231479',\n",
       " '87235637',\n",
       " '87251241',\n",
       " '87252385',\n",
       " '87261426',\n",
       " '87281235',\n",
       " '87290433',\n",
       " '87296136',\n",
       " '87299569',\n",
       " '87316210',\n",
       " '87316220',\n",
       " '87316328',\n",
       " '87324028',\n",
       " '87325497'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stem_inverted[porter.stem('menopausal')] | stem_inverted[porter.stem('pregnant')]) & \\\n",
    " stem_inverted[porter.stem('woman')] - stem_inverted[porter.stem('healthy')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "## Vector Retrieval\n",
    "\n",
    "### Exercise: Boolean Information Retrieval\n",
    "\n",
    "Create an inverted index of the NLTK Gutenberg corpus and save it into a file \"gutenbergindex.pickle\". To create this index there is no need to look for stop words or word frequencies, since the corpus is not that large. Simply use all the words. Use this index to find the documents that match the following Boolean queries:\n",
    "\n",
    "1. Brutus OR Caesar\n",
    "2. Brutus AND NOT Caesar\n",
    "3. (Brutus AND Caesar) OR Calpurnia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: tf.idf\n",
    "\n",
    "Using scikit-learn, compute the tf.idf of all words in the OHSUMED corpus. Use the English list of stop words, and all other settings should use the default values. In particular, do not stem the words. Pickle the resulting tf.idf vectoriser into a file tfidf.pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sort by tf.idf\n",
    "\n",
    "Write a program that prints the words of a document with highest tf.idf score, sorted in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: frequency vs. tf.idf\n",
    "\n",
    "Sort the words by frequency and plot their tf.idf score (we have seen this in the lectures). Can you see any interesting patterns in the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: tf.idf cosine similarity\n",
    "\n",
    "Write a function that takes as a parameter a string and an optional parameter n the number of results, and returns the IDs of the n documents that are most relevant according to the tf.idf measure and the cosine similarity. The results are sorted in descending order of the cosine similarity score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_documents(querystring,n=10):\n",
    "    \"Return the best documents sorted by relevance\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank\n",
    "\n",
    "The following graph represents a tiny collection of HTML documents and how they are linked. You will implement PageRank on the documents.\n",
    "\n",
    "![pagerank](graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Transition matrix\n",
    "\n",
    "Write the transition matrix using Numpy Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Iteration of PageRank\n",
    "\n",
    "Perform an iteration of the PageRank algorithm and report on the resulting PageRank scores of each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Complete PageRank\n",
    "\n",
    "Perform the PageRank algorithm until convergence and report the number of iterations and the final scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
