{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration\n",
    "\n",
    "The following demonstration will use the training set of the OHSUMED corpus. This training set was used in the Filtering Track of the 9th edition of the Text REtrieval Conference (TREC-9). We will use it for the information retrieval exercises of this workshop. Download and unzip [ohsumed.zip](ohsumed.zip) into the same folder as this notebook. \n",
    "\n",
    "To help you read the data, we are providing the file ohsumed.py (in the zip file above) that has a simple API to the data. When you import it at the Python prompt, it will provide the following variables:\n",
    "\n",
    "\n",
    "1. `index`: a dictionary with document IDs as keys, and document text as values.\n",
    "2. `questions`: a dictionary with query IDs as keys, and query text as values.\n",
    "3. `answers`: a dictionary with query IDs as keys, and a set with the IDs of known relevant documents for evaluation purposes.\n",
    "\n",
    "Below are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading OHSUMED data\n"
     ]
    }
   ],
   "source": [
    "import ohsumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54710"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['87049087',\n",
       " '87049088',\n",
       " '87049089',\n",
       " '87049090',\n",
       " '87049092',\n",
       " '87049093',\n",
       " '87049094',\n",
       " '87049096',\n",
       " '87049098',\n",
       " '87049099']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ohsumed.index.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plasma cholecystokinin (CCK) responses after ingestion of a test meal in patients with mild chronic pancreatitis having abdominal pain were studied with a radioimmunoassay using the CCK specific antiserum (OAL-656) produced by a novel immunization procedure. Mean concentration of the fasting plasma CCK determined using CCK-8 as a standard was 31.5 +/- 5.8 pg/ml in six patients who had mild impaired exocrine function with pain, and was significantly higher than 10 healthy subjects (9.8 +/- 1.8 pg/ml). In those patients, the ingestion of a liquid test meal led to a peak of 75.1 +/- 25.4 pg/ml at 30 min, and the 120-min integrated CCK response (5427 +/- 1217.3 pg X min/ml) was significantly higher than in healthy subjects (1538 +/- 110.1 pg X min/ml).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohsumed.index['87073365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OHSU1',\n",
       " 'OHSU2',\n",
       " 'OHSU3',\n",
       " 'OHSU4',\n",
       " 'OHSU5',\n",
       " 'OHSU6',\n",
       " 'OHSU7',\n",
       " 'OHSU8',\n",
       " 'OHSU9',\n",
       " 'OHSU10']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ohsumed.questions.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'young wf with lactase deficiency lactase deficiency therapy options'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohsumed.questions['OHSU7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87103870', '87104937', '87124599', '87153184', '87253647', '87267477'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohsumed.answers['OHSU7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index\n",
    "\n",
    "We are going to build an inverted index of the non-stop words with frequency higher than 5.\n",
    "\n",
    "The following code reads the files and creates a counter of all words in the corpus (including stop words). We will use NLTK's word tokeniser (read the beginning of [chapter 3 of NLTK's book](http://www.nltk.org/book/ch03.html#processing-raw-text)) to convert each document into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk, collections\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "wordcounter = collections.Counter([w.lower() for k in ohsumed.index\n",
    "                                             for s in nltk.sent_tokenize(ohsumed.index[k])\n",
    "                                             for w in nltk.word_tokenize(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 305806),\n",
       " ('of', 271953),\n",
       " ('.', 254858),\n",
       " (',', 239656),\n",
       " ('and', 179604),\n",
       " ('in', 172449),\n",
       " ('to', 107431),\n",
       " (')', 96259),\n",
       " ('(', 95948),\n",
       " ('a', 95277)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the inverted index of all non-stop words with frequency higher than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inverted = dict()\n",
    "for d in ohsumed.index:\n",
    "    for w in nltk.word_tokenize(ohsumed.index[d]):\n",
    "        w = w.lower()\n",
    "        if w in stop or wordcounter[w] <= 5:\n",
    "            continue\n",
    "        if w in inverted:\n",
    "            inverted[w].add(d)\n",
    "        else:\n",
    "            inverted[w] = set([d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptably',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'acceptor',\n",
       " 'acceptors',\n",
       " 'access']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(inverted.keys()))[3000:3010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87057543',\n",
       " '87067994',\n",
       " '87073895',\n",
       " '87074134',\n",
       " '87114326',\n",
       " '87119697',\n",
       " '87121859',\n",
       " '87129900',\n",
       " '87149032',\n",
       " '87153185',\n",
       " '87193350',\n",
       " '87223625',\n",
       " '87223856',\n",
       " '87224779',\n",
       " '87232524',\n",
       " '87251875',\n",
       " '87273001',\n",
       " '87282178',\n",
       " '87295871',\n",
       " '87297008'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted['acceptability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code saves the inverted index into a pickle file. This way we do not need to recompute the inverted index again. Read [Python's documentation on pickle files](https://docs.python.org/3/library/pickle.html) for more detail. Note that the file we created is opened for writing in binary mode, following the advice of a [stackoverflow post about saving pickle files](http://stackoverflow.com/questions/13906623/using-pickle-dump-typeerror-must-be-str-not-bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inverted.pickle','wb') as f:\n",
    "    pickle.dump(inverted,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads the pickled file and returns the list of documents that maches this Boolean query:\n",
    "1. (menopausal OR pregnant) AND woman AND NOT healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('inverted.pickle','rb') as f:\n",
    "    inverted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87060673',\n",
       " '87066899',\n",
       " '87097274',\n",
       " '87097518',\n",
       " '87099263',\n",
       " '87114245',\n",
       " '87117852',\n",
       " '87128881',\n",
       " '87134330',\n",
       " '87138205',\n",
       " '87153548',\n",
       " '87153568',\n",
       " '87169457',\n",
       " '87185313',\n",
       " '87226668',\n",
       " '87231479',\n",
       " '87235637',\n",
       " '87251241',\n",
       " '87252385',\n",
       " '87261426',\n",
       " '87281235',\n",
       " '87290433',\n",
       " '87296136',\n",
       " '87316210',\n",
       " '87316220',\n",
       " '87316328',\n",
       " '87324028',\n",
       " '87325497'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inverted['menopausal'] | inverted['pregnant']) & inverted['woman'] - inverted['healthy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "## Vector Retrieval\n",
    "\n",
    "### Exercise: Boolean Information Retrieval\n",
    "\n",
    "Create an inverted index of the NLTK Gutenberg corpus and save it into a file \"gutenbergindex.pickle\". To create this index there is no need to look for stop words or word frequencies, since the corpus is not that large. Simply use all the words. Use this index to find the IDS of the documents that match the following Boolean queries:\n",
    "\n",
    "1. Brutus OR Caesar\n",
    "2. Brutus AND NOT Caesar\n",
    "3. (Brutus AND Caesar) OR Calpurnia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "austen-emma.txt\n",
      "austen-persuasion.txt\n",
      "austen-sense.txt\n",
      "bible-kjv.txt\n",
      "blake-poems.txt\n",
      "bryant-stories.txt\n",
      "burgess-busterbrown.txt\n",
      "carroll-alice.txt\n",
      "chesterton-ball.txt\n",
      "chesterton-brown.txt\n",
      "chesterton-thursday.txt\n",
      "edgeworth-parents.txt\n",
      "melville-moby_dick.txt\n",
      "milton-paradise.txt\n",
      "shakespeare-caesar.txt\n",
      "shakespeare-hamlet.txt\n",
      "shakespeare-macbeth.txt\n",
      "whitman-leaves.txt\n",
      "Saving index into file gutenbergindex.pickle...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "nltk.download(\"gutenberg\")\n",
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('gutenbergindex.pickle','rb') as f:\n",
    "    gutenbergindex = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bible-kjv.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code for searching for Brutus OR Caesar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code for searching for Brutus AND NOT Caesar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Calpurnia'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8203fe9d19ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgutenbergindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Brutus'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mgutenbergindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Caesar'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgutenbergindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Calpurnia'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Calpurnia'"
     ]
    }
   ],
   "source": [
    "# Write your code for searching for (Brutus AND Caesar) OR Calpurnia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_It turns out that 'Calpurnia' is not indexed. Can you figure out what sort of error handling you could do to handle words that are not indexed?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: tf.idf\n",
    "\n",
    "Using scikit-learn, compute the tf.idf of all words in the OHSUMED corpus. Use the English list of stop words, and leave all other settings to their default values. In particular, do not stem the words. Pickle the resulting tf.idf vectoriser into a file tfidf.pickle. *Note that in this exercise you should use the sklearn functions, not nltk. In particular, do not use NLTK's list of stop words or its tokeniser.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code to compute the tf.idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code to save the results in a pickle file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sort by tf.idf\n",
    "\n",
    "Write a program that prints the words of a document with highest tf.idf score, sorted in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_tfidf(tfidf,docID,numwords=10):\n",
    "    \"Print the words with highest tf.idf, in descending order\"\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rhythms',\n",
       " 'refibrillation',\n",
       " 'organized',\n",
       " 'refibrillated',\n",
       " 'converted',\n",
       " 'emt',\n",
       " 'paramedic',\n",
       " 'ds',\n",
       " 'defibrillation',\n",
       " 'hospital']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_tfidf(tfidf,'87049087')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: tf.idf cosine similarity\n",
    "\n",
    "Write a function that takes as a parameter a string and an optional parameter $n$ the number of results, and returns the IDs of the $n$ documents that are most relevant according to the tf.idf vector of the documents and cosine similarity. The results are sorted in descending order of the cosine similarity score. *If the execution of your program takes too long, in your solution you can process only the first 1000 files from the ohsumed collection.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_documents(querystring,n=10):\n",
    "    \"Return the best documents sorted by relevance\"\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['87052846',\n",
       " '87053030',\n",
       " '87057603',\n",
       " '87057561',\n",
       " '87054719',\n",
       " '87053640',\n",
       " '87053630',\n",
       " '87055106',\n",
       " '87057550',\n",
       " '87053614']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_documents(ohsumed.questions['OHSU1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank\n",
    "\n",
    "The following graph represents a tiny collection of HTML documents and how they are linked. You will implement PageRank on the documents.\n",
    "\n",
    "![pagerank](graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Transition matrix\n",
    "\n",
    "Write the transition matrix using Numpy Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: An Iteration of PageRank\n",
    "\n",
    "Perform an iteration of the PageRank algorithm and report on the resulting PageRank scores of each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25   ],\n",
       "       [ 0.14375],\n",
       "       [ 0.56875],\n",
       "       [ 0.0375 ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Complete PageRank\n",
    "\n",
    "Perform the PageRank algorithm until convergence and report the number of iterations and the final scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR after 9 iterations:\n",
      "[[ 0.37044014]\n",
      " [ 0.19493706]\n",
      " [ 0.3971228 ]\n",
      " [ 0.0375    ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
