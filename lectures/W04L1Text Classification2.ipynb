{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"movie_reviews\")\n",
    "from nltk.corpus import movie_reviews\n",
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative reviews: 1000\n",
      "Number of positive reviews: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of negative reviews:\", len(movie_reviews.fileids('neg')))\n",
    "print(\"Number of positive reviews:\", len(movie_reviews.fileids('pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code partitions the movie review corpus into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.seed(1234)\n",
    "random.shuffle(documents)\n",
    "threshold1 = int(len(documents)*.6)\n",
    "threshold2 = int(len(documents)*.8)\n",
    "train = documents[:threshold1]\n",
    "devtest = documents[threshold1:threshold2]\n",
    "test = documents[threshold2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code finds the 2000 most frequent non-stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "c = collections.Counter([w.lower() for (words,category) in train \n",
    "                                   for w in words if w.lower() not in stop])\n",
    "top2000words = [w for (w,count) in c.most_common(2000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements one-hot encoding with the 2000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(words):\n",
    "    \"Return the document features for an NLTK classifier\"\n",
    "    words_lower = [w.lower() for w in words]\n",
    "    result = dict()\n",
    "    for w in top2000words:\n",
    "        result['has(%s)' % w] = (w in words_lower)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we train an NLTK Naive Bayes classifier using the training set, and evaluate the system using the devtest set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = [(document_features(x), y) for (x, y) in train]\n",
    "devtest_features = [(document_features(x), y) for (x, y) in devtest]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7775"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, devtest_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8816666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the difference in accuracy between the test set and the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a second feature extractor that uses one-hot encoding on the same list of 2000 words, and which is suitable for sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_features(words):\n",
    "    \"Return a vector of features for sklearn\"\n",
    "    words_lower = [w.lower() for w in words]\n",
    "    result = []\n",
    "    for w in top2000words:\n",
    "        if w in words_lower:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code that generates the vectors, trains a Multinomial Naive Bayes classifier, and evaluates the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vectors = [vector_features(x) for (x, y) in train]\n",
    "train_labels = [y for (x, y) in train]\n",
    "devtest_vectors = [vector_features(x) for (x, y) in devtest]\n",
    "devtest_labels = [y for (x, y) in devtest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "sklearn_classifier = MultinomialNB()\n",
    "sklearn_classifier.fit(train_vectors, [y for (x, y) in train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83999999999999997"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = sklearn_classifier.predict(devtest_vectors)\n",
    "accuracy_score(devtest_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92000000000000004"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = sklearn_classifier.predict(train_vectors)\n",
    "accuracy_score(train_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below is the code that uses Support Vector Machines (SVM) instead. You can see that the interface is the same. SVMs typically give very good results, especially when the amount of training data is large enough (in this case it wasn't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "sklearn_classifier2 = SVC()\n",
    "sklearn_classifier2.fit(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77000000000000002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = sklearn_classifier2.predict(devtest_vectors)\n",
    "accuracy_score(devtest_labels, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84333333333333338"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = sklearn_classifier2.predict(train_vectors)\n",
    "accuracy_score(train_labels, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold Cross Validation using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8136646 ,  0.75776398,  0.8447205 ,  0.83125   ,  0.825     ,\n",
       "        0.825     ,  0.825     ,  0.81761006,  0.77987421,  0.79874214])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "crossval_classifier = SVC()\n",
    "dev_vectors = train_vectors + devtest_vectors\n",
    "dev_labels = train_labels + devtest_labels\n",
    "scores = cross_val_score(crossval_classifier, dev_vectors, dev_labels, cv=10, scoring=\"accuracy\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of accuracy: 0.811862548342\n",
      "Standard deviation of accuracy: 0.0247621315653\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of accuracy:\", scores.mean())\n",
    "print(\"Standard deviation of accuracy:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Accuracy: 0.775\n",
      "Fold 1:\n",
      "Accuracy: 0.812\n",
      "Fold 2:\n",
      "Accuracy: 0.800\n",
      "Fold 3:\n",
      "Accuracy: 0.806\n",
      "Fold 4:\n",
      "Accuracy: 0.831\n",
      "Fold 5:\n",
      "Accuracy: 0.812\n",
      "Fold 6:\n",
      "Accuracy: 0.819\n",
      "Fold 7:\n",
      "Accuracy: 0.825\n",
      "Fold 8:\n",
      "Accuracy: 0.812\n",
      "Fold 9:\n",
      "Accuracy: 0.838\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "dev_array_vectors = np.array(dev_vectors)\n",
    "dev_array_labels = np.array(dev_labels)\n",
    "fold = 0\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "for kv_train, kv_test in kf.split(dev_vectors):\n",
    "    # kv_train and kv_test are indices of array dev_vectors\n",
    "    print(\"Fold %i:\" % fold)\n",
    "    fold += 1\n",
    "    cv_classifier = SVC()\n",
    "    cv_classifier.fit(dev_array_vectors[kv_train], dev_array_labels[kv_train])\n",
    "    test_predictions = cv_classifier.predict(dev_array_vectors[kv_test])\n",
    "    test_accuracy = accuracy_score(dev_array_labels[kv_test], test_predictions)\n",
    "    print(\"Accuracy: %.3f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code splits the Brown corpus into a training and test set. Note that now we cannot shuffle the sentences since we will need information from text from previous and following sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"brown\")\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = brown.sents(categories='news')\n",
    "size = int(len(sents)*0.1)\n",
    "train_sents, test_sents = sents[size:], sents[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code extracts the boundary information of tokenised sentences. This can be used for our annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_boundaries(sents):\n",
    "    \"\"\"Return the tokens and the sentence boundary positions\"\"\"\n",
    "    tokens = []\n",
    "    boundaries = []\n",
    "    offset = 0\n",
    "    for sent in sents:\n",
    "        tokens.extend(sent)\n",
    "        offset += len(sent)\n",
    "        boundaries.append(offset-1)\n",
    "    return tokens, boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tokens, train_boundaries = extract_boundaries(train_sents)\n",
    "test_tokens, test_boundaries = extract_boundaries(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'assured',\n",
       " 'Mr.',\n",
       " 'Martinelli',\n",
       " 'and',\n",
       " 'the',\n",
       " 'council',\n",
       " 'that',\n",
       " 'he',\n",
       " 'would',\n",
       " 'study',\n",
       " 'the',\n",
       " 'correct',\n",
       " 'method',\n",
       " 'and',\n",
       " 'report',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'council',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'possible',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Martinelli',\n",
       " 'said',\n",
       " 'yesterday',\n",
       " 'that',\n",
       " 'the',\n",
       " 'Citizens',\n",
       " 'Group',\n",
       " 'of',\n",
       " 'Johnston',\n",
       " 'will',\n",
       " 'meet',\n",
       " 'again',\n",
       " 'July',\n",
       " '24',\n",
       " 'to',\n",
       " 'plan',\n",
       " 'further',\n",
       " 'strategy',\n",
       " 'in',\n",
       " 'the',\n",
       " 'charter',\n",
       " 'movement',\n",
       " '.',\n",
       " 'He']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 48, 77, 96, 115, 149, 181, 202, 239, 252]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_boundaries[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soon', 'as', 'possible', '.', 'Mr.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[21:26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define context-based features for all tokens that are candidates to sentence endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segmenter_features(tokens, i):\n",
    "    \"\"\"Return the features of token[i]\"\"\"\n",
    "    return {'next-word-capitalized': \n",
    "                  tokens[i+1][0].isupper(),\n",
    "            'prev-word': tokens[i-1].lower(),\n",
    "            'punct': tokens[i],\n",
    "            'prev-word-is-one-char': \n",
    "                  len(tokens[i-1]) == 1}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tokens, boundaries, and feature extractor, we can prepare the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = '.?!'\n",
    "train_features = [(segmenter_features(train_tokens, i), \n",
    "                   (i in train_boundaries))\n",
    "                 for i in range(1, len(train_tokens)-1)\n",
    "                 if train_tokens[i] in candidates]\n",
    "test_features = [(segmenter_features(test_tokens, i), \n",
    "                  (i in test_boundaries))\n",
    "                 for i in range(1, len(test_tokens)-1)\n",
    "                 if test_tokens[i] in candidates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'next-word-capitalized': True,\n",
       "   'prev-word': 'possible',\n",
       "   'prev-word-is-one-char': False,\n",
       "   'punct': '.'},\n",
       "  True),\n",
       " ({'next-word-capitalized': True,\n",
       "   'prev-word': 'movement',\n",
       "   'prev-word-is-one-char': False,\n",
       "   'punct': '.'},\n",
       "  True),\n",
       " ({'next-word-capitalized': False,\n",
       "   'prev-word': 'comes',\n",
       "   'prev-word-is-one-char': False,\n",
       "   'punct': '.'},\n",
       "  True)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3749, 407)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features), len(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a classifier that can be used for sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmenter = nltk.NaiveBayesClassifier.train(train_features)\n",
    "nltk.classify.accuracy(segmenter, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks impressive! but let's check what would happen if we introduced a majority baseline classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 62, True: 3687})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "train_counter = Counter([f[1] for f in train_features])\n",
    "train_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most training samples are labelled as `True,` the majority baseline is a classifier that always outputs `True`. In that case, accuracy in the test set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 2, True: 405})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counter = Counter([f[1] for f in test_features])\n",
    "test_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995085995085995"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "405/407"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the majority baseline was not that impressive after all. The finished segmenter that uses the trained classifier is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_sentences(tokens):\n",
    "    \"\"\"Segment a list of tokens\"\"\"\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in candidates and \\\n",
    "           segmenter.classify(segmenter_features(tokens, i)) == True:\n",
    "               sents.append(tokens[start:i+1])\n",
    "               start = i+1\n",
    "    if start < len(tokens):\n",
    "        sents.append(tokens[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'a', 'sentence', '.'], ['This', 'is', 'another', 'one']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_sentences([\"This\", \"is\", \"a\", \"sentence\", \".\", \"This\", \n",
    "                    \"is\", \"another\", \"one\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reuters-21578 Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"reuters\")\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acq',\n",
       " 'alum',\n",
       " 'barley',\n",
       " 'bop',\n",
       " 'carcass',\n",
       " 'castor-oil',\n",
       " 'cocoa',\n",
       " 'coconut',\n",
       " 'coconut-oil',\n",
       " 'coffee',\n",
       " 'copper',\n",
       " 'copra-cake',\n",
       " 'corn',\n",
       " 'cotton',\n",
       " 'cotton-oil',\n",
       " 'cpi',\n",
       " 'cpu',\n",
       " 'crude',\n",
       " 'dfl',\n",
       " 'dlr',\n",
       " 'dmk',\n",
       " 'earn',\n",
       " 'fuel',\n",
       " 'gas',\n",
       " 'gnp',\n",
       " 'gold',\n",
       " 'grain',\n",
       " 'groundnut',\n",
       " 'groundnut-oil',\n",
       " 'heat',\n",
       " 'hog',\n",
       " 'housing',\n",
       " 'income',\n",
       " 'instal-debt',\n",
       " 'interest',\n",
       " 'ipi',\n",
       " 'iron-steel',\n",
       " 'jet',\n",
       " 'jobs',\n",
       " 'l-cattle',\n",
       " 'lead',\n",
       " 'lei',\n",
       " 'lin-oil',\n",
       " 'livestock',\n",
       " 'lumber',\n",
       " 'meal-feed',\n",
       " 'money-fx',\n",
       " 'money-supply',\n",
       " 'naphtha',\n",
       " 'nat-gas',\n",
       " 'nickel',\n",
       " 'nkr',\n",
       " 'nzdlr',\n",
       " 'oat',\n",
       " 'oilseed',\n",
       " 'orange',\n",
       " 'palladium',\n",
       " 'palm-oil',\n",
       " 'palmkernel',\n",
       " 'pet-chem',\n",
       " 'platinum',\n",
       " 'potato',\n",
       " 'propane',\n",
       " 'rand',\n",
       " 'rape-oil',\n",
       " 'rapeseed',\n",
       " 'reserves',\n",
       " 'retail',\n",
       " 'rice',\n",
       " 'rubber',\n",
       " 'rye',\n",
       " 'ship',\n",
       " 'silver',\n",
       " 'sorghum',\n",
       " 'soy-meal',\n",
       " 'soy-oil',\n",
       " 'soybean',\n",
       " 'strategic-metal',\n",
       " 'sugar',\n",
       " 'sun-meal',\n",
       " 'sun-oil',\n",
       " 'sunseed',\n",
       " 'tea',\n",
       " 'tin',\n",
       " 'trade',\n",
       " 'veg-oil',\n",
       " 'wheat',\n",
       " 'wpi',\n",
       " 'yen',\n",
       " 'zinc']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/14832',\n",
       " 'test/14858',\n",
       " 'test/15033',\n",
       " 'test/15043',\n",
       " 'test/15106',\n",
       " 'test/15287',\n",
       " 'test/15341',\n",
       " 'test/15618',\n",
       " 'test/15648',\n",
       " 'test/15676',\n",
       " 'test/15686',\n",
       " 'test/15720',\n",
       " 'test/15845',\n",
       " 'test/15856',\n",
       " 'test/15860',\n",
       " 'test/15863',\n",
       " 'test/15871',\n",
       " 'test/15875',\n",
       " 'test/15877',\n",
       " 'test/15890',\n",
       " 'test/15904',\n",
       " 'test/15906',\n",
       " 'test/15910',\n",
       " 'test/15911',\n",
       " 'test/15917',\n",
       " 'test/15952',\n",
       " 'test/15999',\n",
       " 'test/16012',\n",
       " 'test/16071',\n",
       " 'test/16099',\n",
       " 'test/16147',\n",
       " 'test/16525',\n",
       " 'test/16624',\n",
       " 'test/16751',\n",
       " 'test/16765',\n",
       " 'test/17503',\n",
       " 'test/17509',\n",
       " 'test/17722',\n",
       " 'test/18035',\n",
       " 'test/18482',\n",
       " 'test/18614',\n",
       " 'test/18954',\n",
       " 'test/18973',\n",
       " 'test/19165',\n",
       " 'test/19721',\n",
       " 'test/19821',\n",
       " 'test/20018',\n",
       " 'test/20366',\n",
       " 'test/20637',\n",
       " 'test/20645',\n",
       " 'test/20649',\n",
       " 'test/20723',\n",
       " 'test/20763',\n",
       " 'test/21091',\n",
       " 'test/21243',\n",
       " 'test/21493',\n",
       " 'training/10120',\n",
       " 'training/10139',\n",
       " 'training/10172',\n",
       " 'training/10175',\n",
       " 'training/10319',\n",
       " 'training/10339',\n",
       " 'training/10487',\n",
       " 'training/10489',\n",
       " 'training/10519',\n",
       " 'training/10701',\n",
       " 'training/10882',\n",
       " 'training/10956',\n",
       " 'training/11012',\n",
       " 'training/11085',\n",
       " 'training/11091',\n",
       " 'training/11269',\n",
       " 'training/1131',\n",
       " 'training/11392',\n",
       " 'training/11436',\n",
       " 'training/11607',\n",
       " 'training/11612',\n",
       " 'training/11729',\n",
       " 'training/11739',\n",
       " 'training/11769',\n",
       " 'training/11885',\n",
       " 'training/11936',\n",
       " 'training/11939',\n",
       " 'training/11964',\n",
       " 'training/12002',\n",
       " 'training/12052',\n",
       " 'training/12055',\n",
       " 'training/1215',\n",
       " 'training/12160',\n",
       " 'training/12311',\n",
       " 'training/12323',\n",
       " 'training/12372',\n",
       " 'training/12417',\n",
       " 'training/12436',\n",
       " 'training/12500',\n",
       " 'training/12583',\n",
       " 'training/12587',\n",
       " 'training/1268',\n",
       " 'training/1273',\n",
       " 'training/12872',\n",
       " 'training/13173',\n",
       " 'training/13179',\n",
       " 'training/1369',\n",
       " 'training/1385',\n",
       " 'training/13852',\n",
       " 'training/13856',\n",
       " 'training/1395',\n",
       " 'training/1399',\n",
       " 'training/14483',\n",
       " 'training/1582',\n",
       " 'training/1652',\n",
       " 'training/1777',\n",
       " 'training/1843',\n",
       " 'training/193',\n",
       " 'training/1952',\n",
       " 'training/197',\n",
       " 'training/2044',\n",
       " 'training/2172',\n",
       " 'training/2183',\n",
       " 'training/2217',\n",
       " 'training/2264',\n",
       " 'training/235',\n",
       " 'training/2382',\n",
       " 'training/2436',\n",
       " 'training/2456',\n",
       " 'training/2595',\n",
       " 'training/2599',\n",
       " 'training/2617',\n",
       " 'training/2727',\n",
       " 'training/2741',\n",
       " 'training/2749',\n",
       " 'training/2777',\n",
       " 'training/2848',\n",
       " 'training/2913',\n",
       " 'training/2922',\n",
       " 'training/2947',\n",
       " 'training/3138',\n",
       " 'training/3191',\n",
       " 'training/327',\n",
       " 'training/3282',\n",
       " 'training/3299',\n",
       " 'training/3306',\n",
       " 'training/3330',\n",
       " 'training/3337',\n",
       " 'training/3358',\n",
       " 'training/3401',\n",
       " 'training/3429',\n",
       " 'training/3847',\n",
       " 'training/3855',\n",
       " 'training/3881',\n",
       " 'training/3949',\n",
       " 'training/395',\n",
       " 'training/3979',\n",
       " 'training/3981',\n",
       " 'training/4047',\n",
       " 'training/4133',\n",
       " 'training/4289',\n",
       " 'training/4296',\n",
       " 'training/4382',\n",
       " 'training/4490',\n",
       " 'training/4599',\n",
       " 'training/4825',\n",
       " 'training/4905',\n",
       " 'training/4939',\n",
       " 'training/4988',\n",
       " 'training/5',\n",
       " 'training/5003',\n",
       " 'training/501',\n",
       " 'training/5017',\n",
       " 'training/5033',\n",
       " 'training/5109',\n",
       " 'training/516',\n",
       " 'training/5185',\n",
       " 'training/5338',\n",
       " 'training/5467',\n",
       " 'training/5518',\n",
       " 'training/5531',\n",
       " 'training/5606',\n",
       " 'training/5610',\n",
       " 'training/5636',\n",
       " 'training/5637',\n",
       " 'training/57',\n",
       " 'training/5847',\n",
       " 'training/5933',\n",
       " 'training/6',\n",
       " 'training/6142',\n",
       " 'training/6221',\n",
       " 'training/6236',\n",
       " 'training/6239',\n",
       " 'training/6259',\n",
       " 'training/6269',\n",
       " 'training/6386',\n",
       " 'training/6585',\n",
       " 'training/6588',\n",
       " 'training/6735',\n",
       " 'training/6890',\n",
       " 'training/6897',\n",
       " 'training/694',\n",
       " 'training/7062',\n",
       " 'training/7205',\n",
       " 'training/7215',\n",
       " 'training/7336',\n",
       " 'training/7387',\n",
       " 'training/7389',\n",
       " 'training/7390',\n",
       " 'training/7395',\n",
       " 'training/7700',\n",
       " 'training/7792',\n",
       " 'training/7917',\n",
       " 'training/7934',\n",
       " 'training/7943',\n",
       " 'training/8004',\n",
       " 'training/8140',\n",
       " 'training/8161',\n",
       " 'training/8166',\n",
       " 'training/8257',\n",
       " 'training/8273',\n",
       " 'training/8400',\n",
       " 'training/8443',\n",
       " 'training/8446',\n",
       " 'training/8535',\n",
       " 'training/855',\n",
       " 'training/8759',\n",
       " 'training/8941',\n",
       " 'training/8983',\n",
       " 'training/8993',\n",
       " 'training/9058',\n",
       " 'training/9093',\n",
       " 'training/9094',\n",
       " 'training/934',\n",
       " 'training/9470',\n",
       " 'training/9521',\n",
       " 'training/9667',\n",
       " 'training/97',\n",
       " 'training/9865',\n",
       " 'training/9958',\n",
       " 'training/9989']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.fileids(categories='corn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corn_fileids = reuters.fileids(categories='corn')\n",
    "gold_fileids = reuters.fileids(categories='gold')\n",
    "grain_fileids = reuters.fileids(categories='grain')\n",
    "\n",
    "training_fileids = [f for f in reuters.fileids() if f[0:8]=='training']\n",
    "testing_fileids = [f for f in reuters.fileids() if f[0:4]=='test']\n",
    "\n",
    "train_fileids_tagged = [(f,'corn') for f in corn_fileids if f[0:8]=='training']\n",
    "train_fileids_tagged += [(f,'gold') for f in gold_fileids if f[0:8]=='training']\n",
    "train_fileids_tagged += [(f,'grain') for f in grain_fileids if f[0:8]=='training']\n",
    "\n",
    "\n",
    "test_fileids_tagged = [(f,'corn') for f in corn_fileids if f[0:4]=='test']\n",
    "test_fileids_tagged += [(f,'gold') for f in gold_fileids if f[0:4]=='test']\n",
    "test_fileids_tagged += [(f,'grain') for f in grain_fileids if f[0:4]=='test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grain_fileids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', ',', 'the']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "all_words = collections.Counter(w.lower() \\\n",
    "        for w in reuters.words(fileids=training_fileids))\n",
    "word_features = [w for (w, c) in all_words.most_common(500)]\n",
    "word_features[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def document_features(fileid):\n",
    "    document_words = set(reuters.words(fileids=[fileid]))\n",
    "    features = dict()\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "train_set = [(document_features(f),t) for (f,t) in train_fileids_tagged]\n",
    "test_set = [(document_features(f),t) for (f,t) in test_fileids_tagged]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          contains(gold) = True             gold : grain  =    278.7 : 1.0\n",
      "             contains(>) = True             gold : grain  =    129.4 : 1.0\n",
      "             contains(&) = True             gold : grain  =     83.1 : 1.0\n",
      "            contains(lt) = True             gold : grain  =     83.1 : 1.0\n",
      "             contains(;) = True             gold : grain  =     59.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6723404255319149"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier,test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro-averaged Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(y_true,y_pred,label):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == label:\n",
    "            if y_pred[i] == label:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        elif y_pred[i] == label:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    try:\n",
    "        r = tp/(tp+fn)\n",
    "    except:\n",
    "        r = 0.0\n",
    "    try:\n",
    "        p = tp/(tp+fp)\n",
    "    except:\n",
    "        p = 0.0\n",
    "    try:\n",
    "        f1 = 2*r*p/(r+p)\n",
    "    except:\n",
    "        f1 = 0.0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corn',\n",
       " 'grain',\n",
       " 'grain',\n",
       " 'corn',\n",
       " 'grain',\n",
       " 'corn',\n",
       " 'grain',\n",
       " 'grain',\n",
       " 'grain',\n",
       " 'grain']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [classifier.classify(f) for f, l in test_set]\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corn',\n",
       " 'corn',\n",
       " 'corn',\n",
       " 'corn',\n",
       " 'corn',\n",
       " 'corn',\n",
       " 'corn',\n",
       " 'corn',\n",
       " 'corn',\n",
       " 'corn']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [l for f, l in test_set]\n",
    "y_true[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corn f1: 0.3894\n",
      "gold f1: 0.8235\n",
      "grain f1: 0.7516\n",
      "Macro-average f1: 0.6548\n"
     ]
    }
   ],
   "source": [
    "totalf1 = 0\n",
    "for label in ('corn','gold','grain'):\n",
    "    thef1 = f1(y_true,predictions,label)\n",
    "    print('%s f1: %1.4f' % (label,thef1))\n",
    "    totalf1 += thef1\n",
    "print(\"Macro-average f1: %1.4f\" % (totalf1/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-averaged Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f1_micro(y_true,y_pred):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    labels = list(set(y_true))\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for label in labels:\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] == label:\n",
    "                if y_pred[i] == label:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            elif y_pred[i] == label:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    try:\n",
    "        r = tp/(tp+fn)\n",
    "    except:\n",
    "        r = 0.0\n",
    "    try:\n",
    "        p = tp/(tp+fp)\n",
    "    except:\n",
    "        p = 0.0\n",
    "    try:\n",
    "        f1 = 2*r*p/(r+p)\n",
    "    except:\n",
    "        f1 = 0.0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average f1: 0.6723\n"
     ]
    }
   ],
   "source": [
    "print(\"Micro-average f1: %1.4f\" % f1_micro(y_true,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6548479765554206"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67234042553191486"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, predictions, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
